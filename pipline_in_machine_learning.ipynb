{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipline in Machine Learning\n",
    "\n",
    "In machine learning, a pipleine is a sequence of data processing steps that are chained together to automate and streamline the machine learning workflow. A pipeline allows you to cmbine multiple data preprocessing and model training steps into a single object, making it easier to organize and manage your machine learning code\n",
    "\n",
    "**Here are the key components of a pipline:**\n",
    "\n",
    "``` **Data preprocessing steps:** ``` pipelines typically start with data preprocessing steps, such as feature scalling, featrue encoding, handling missing values, or dimensionality reduction. These steps ensure that the data is in the appropriate format and quality for model training.\n",
    "\n",
    "**Model Training:** After the data preprocessing steps, the pipleine includes the training of a machine learning model. This can be a classifier for classification tasks, a regressor for regression tasks, or any other type of model depending on the problem at hand.\n",
    "\n",
    "```**Model Evaluation:**``` Once the model is trained, the pipline often incorporates steps for evaluating its performance. this may involve metrics calculation, cross-validation, or any other evaluation technique to assess the model's effectiveness.\n",
    "\n",
    "```**Prediction:**``` After the model has been evaluated, the pipeline allows you to make predictions on new, unseen data using the trained model. this step applies generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Lab elEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset from seaborn\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# select features and target variables\n",
    "x = df[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# split the data into train test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the column transformer for imputing missing\n",
    "numeric_features = ['age', 'fare']\n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# create a pipeline with the preprocessor and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# fit the pipeline on the training data\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "# calculating accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunning in pipeline\n",
    "\n",
    "Hyperparameter tunning in a pipeline involves optimizing the hyperparamters of the different steps in the pipeline to find the best combination that maximizes the model's performance. here's and example of hyperparameter tunning in a pipeline and selecting the best model on the titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy 1.0\n",
      "Best Hyperparameters {'model__max_depth': None, 'model__min_samples_split': 2, 'model__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset fro seaborn \n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# select features and target variables\n",
    "x = df.drop(\"survived\", axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "#split the data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)\n",
    "\n",
    "#create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "#Define the hyperparameters to tune\n",
    "hyperparameters = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 5, 10],\n",
    "    'model__min_samples_split': [2,5, 10]\n",
    "}\n",
    "\n",
    "#perform grid search cross-validation\n",
    "grid_search = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# make prediction on the test data using the best model\n",
    "y_pred  = best_model.predict(x_test)\n",
    "\n",
    "# calculating the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuaracy\", accuracy)\n",
    "\n",
    "#print the best paramter\n",
    "print(\"Best Hyperparameters\", grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
